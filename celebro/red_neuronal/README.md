# @red_neuronal - Sistema de Redes Neuronales Completo

**Versi√≥n:** 0.6.0  
**Autor:** LucIA Development Team  
**Fecha:** 20 de Septiembre de 2025

## üß† Descripci√≥n

@red_neuronal es un sistema completo de redes neuronales implementado desde cero en Python, dise√±ado para ser modular, eficiente y f√°cil de usar. Forma parte del ecosistema @celebro y est√° integrado con LucIA.

## üéØ Caracter√≠sticas Principales

### üèóÔ∏è Arquitectura Modular
- **Capas personalizables** (Dense, Conv, Pooling, Dropout, BatchNorm)
- **Funciones de activaci√≥n** avanzadas (ReLU, Sigmoid, Tanh, Softmax, LeakyReLU, ELU, Swish, GELU)
- **Optimizadores** modernos (Adam, SGD, RMSprop, Adagrad, AdamW, AdaDelta, Nadam, RAdam)
- **Funciones de p√©rdida** completas (MSE, MAE, Huber, CrossEntropy, FocalLoss, DiceLoss)

### üöÄ Rendimiento Avanzado
- **Entrenamiento eficiente** con mini-lotes
- **Early stopping** autom√°tico
- **Batch normalization** para estabilizaci√≥n
- **Dropout** para regularizaci√≥n
- **Gradient clipping** para estabilidad
- **Learning rate scheduling** din√°mico

### üîß Facilidad de Uso
- **API simple** e intuitiva
- **Configuraci√≥n declarativa** con NetworkConfig
- **Guardado/carga** de modelos
- **Visualizaci√≥n** de m√©tricas
- **Logging** detallado

## üìÅ Estructura del Proyecto

```
@red_neuronal/
‚îú‚îÄ‚îÄ __init__.py                 # Inicializaci√≥n del m√≥dulo
‚îú‚îÄ‚îÄ neural_network.py          # Red neuronal principal
‚îú‚îÄ‚îÄ layers.py                  # Implementaci√≥n de capas
‚îú‚îÄ‚îÄ optimizers.py              # Algoritmos de optimizaci√≥n
‚îú‚îÄ‚îÄ loss_functions.py          # Funciones de p√©rdida
‚îú‚îÄ‚îÄ neural_core.py             # N√∫cleo central del sistema
‚îú‚îÄ‚îÄ training.py                # Sistema de entrenamiento
‚îú‚îÄ‚îÄ evaluation.py              # Sistema de evaluaci√≥n
‚îú‚îÄ‚îÄ visualization.py           # Herramientas de visualizaci√≥n
‚îú‚îÄ‚îÄ models/                    # Modelos guardados
‚îú‚îÄ‚îÄ logs/                      # Logs del sistema
‚îú‚îÄ‚îÄ data/                      # Datos de ejemplo
‚îî‚îÄ‚îÄ README.md                  # Documentaci√≥n
```

## üöÄ Instalaci√≥n y Uso

### Requisitos
```bash
pip install numpy matplotlib scikit-learn
```

### Uso B√°sico
```python
from celebro.red_neuronal import NeuralCore, NetworkConfig, TrainingConfig

# Crear n√∫cleo de red neuronal
neural_core = NeuralCore()

# Configurar red
config = NetworkConfig(
    input_size=784,
    hidden_layers=[128, 64, 32],
    output_size=10,
    activation='relu',
    output_activation='softmax',
    learning_rate=0.001,
    dropout_rate=0.2
)

# Crear red
network = neural_core.create_network(config)

# Configurar entrenamiento
training_config = TrainingConfig(
    epochs=100,
    batch_size=32,
    learning_rate=0.001,
    validation_split=0.2,
    early_stopping=True,
    patience=10
)

# Entrenar
history = neural_core.train(X_train, y_train, config=training_config)

# Evaluar
metrics = neural_core.evaluate(X_test, y_test)

# Predecir
predictions = neural_core.predict(X_new)
```

## üèóÔ∏è Componentes Principales

### 1. NeuralCore
N√∫cleo central que coordina todos los componentes:

```python
neural_core = NeuralCore()

# Crear red
network = neural_core.create_network(config)

# Entrenar
history = neural_core.train(X_train, y_train)

# Evaluar
metrics = neural_core.evaluate(X_test, y_test)

# Guardar modelo
neural_core.save_model("modelo.pkl")

# Cargar modelo
neural_core.load_model("modelo.pkl")
```

### 2. Capas de Red Neuronal

#### DenseLayer (Capa Densa)
```python
from celebro.red_neuronal import DenseLayer

dense = DenseLayer(
    units=128,
    activation='relu',
    kernel_initializer='he_normal',
    use_bias=True
)
```

#### ConvLayer (Capa Convolucional)
```python
from celebro.red_neuronal import ConvLayer

conv = ConvLayer(
    filters=32,
    kernel_size=(3, 3),
    strides=(1, 1),
    padding='valid',
    activation='relu'
)
```

#### DropoutLayer (Regularizaci√≥n)
```python
from celebro.red_neuronal import DropoutLayer

dropout = DropoutLayer(rate=0.5)
```

#### BatchNormLayer (Normalizaci√≥n)
```python
from celebro.red_neuronal import BatchNormLayer

batch_norm = BatchNormLayer(momentum=0.9, epsilon=1e-5)
```

### 3. Optimizadores

#### Adam (Recomendado)
```python
from celebro.red_neuronal import Adam

optimizer = Adam(
    learning_rate=0.001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-8
)
```

#### SGD con Momentum
```python
from celebro.red_neuronal import SGD

optimizer = SGD(
    learning_rate=0.01,
    momentum=0.9,
    nesterov=True
)
```

#### RMSprop
```python
from celebro.red_neuronal import RMSprop

optimizer = RMSprop(
    learning_rate=0.001,
    rho=0.9,
    epsilon=1e-8
)
```

### 4. Funciones de P√©rdida

#### CrossEntropy (Clasificaci√≥n)
```python
from celebro.red_neuronal import CrossEntropy

loss = CrossEntropy(
    from_logits=False,
    label_smoothing=0.0
)
```

#### MSE (Regresi√≥n)
```python
from celebro.red_neuronal import MSE

loss = MSE()
```

#### Huber (Robusta)
```python
from celebro.red_neuronal import Huber

loss = Huber(delta=1.0)
```

#### FocalLoss (Desbalanceado)
```python
from celebro.red_neuronal import FocalLoss

loss = FocalLoss(alpha=1.0, gamma=2.0)
```

## üìä Ejemplos de Uso

### Clasificaci√≥n de Im√°genes
```python
# Crear red convolucional
network = neural_core.create_convolutional_network(
    input_shape=(28, 28, 1),
    num_classes=10,
    config={
        'filters': [32, 64, 128],
        'kernel_sizes': [(3, 3), (3, 3), (3, 3)],
        'pool_sizes': [(2, 2), (2, 2), (2, 2)],
        'dense_units': [128, 64],
        'dropout_rate': 0.5
    }
)

# Entrenar
history = neural_core.train(X_train, y_train)
```

### Regresi√≥n
```python
# Configurar para regresi√≥n
config = NetworkConfig(
    input_size=10,
    hidden_layers=[64, 32, 16],
    output_size=1,
    activation='relu',
    output_activation='linear',
    learning_rate=0.001
)

# Crear y entrenar
network = neural_core.create_network(config)
history = neural_core.train(X_train, y_train)
```

### Clasificaci√≥n Binaria
```python
# Configurar para clasificaci√≥n binaria
config = NetworkConfig(
    input_size=20,
    hidden_layers=[64, 32],
    output_size=2,
    activation='relu',
    output_activation='softmax',
    dropout_rate=0.3
)

# Crear y entrenar
network = neural_core.create_network(config)
history = neural_core.train(X_train, y_train)
```

## üîß Configuraci√≥n Avanzada

### Learning Rate Scheduling
```python
from celebro.red_neuronal import LearningRateScheduler

# Crear scheduler
scheduler = LearningRateScheduler(
    optimizer=optimizer,
    schedule_type='cosine',
    T_max=100,
    eta_min=1e-6
)

# Usar en entrenamiento
for epoch in range(100):
    new_lr = scheduler.step(epoch)
    # Entrenar √©poca...
```

### Gradient Clipping
```python
from celebro.red_neuronal import GradientClipping

# Crear clipper
clipper = GradientClipping(max_norm=1.0, norm_type='l2')

# Aplicar a gradientes
clipped_gradients = clipper.clip_gradients(gradients)
```

### Callbacks Personalizados
```python
class CustomCallback:
    def on_epoch_end(self, epoch, logs):
        print(f"√âpoca {epoch}: {logs}")
    
    def on_training_end(self, logs):
        print("Entrenamiento completado")

# Usar callback
callbacks = [CustomCallback()]
training_config = TrainingConfig(callbacks=callbacks)
```

## üìà Monitoreo y Visualizaci√≥n

### M√©tricas de Entrenamiento
```python
# Obtener historial
history = neural_core.training_history

# Acceder a m√©tricas
for metrics in history:
    print(f"√âpoca {metrics.epoch}:")
    print(f"  P√©rdida: {metrics.train_loss:.4f}")
    print(f"  Precisi√≥n: {metrics.train_accuracy:.4f}")
    print(f"  Val. P√©rdida: {metrics.val_loss:.4f}")
    print(f"  Val. Precisi√≥n: {metrics.val_accuracy:.4f}")
```

### Resumen de la Red
```python
# Obtener resumen de arquitectura
summary = neural_core.get_network_summary()
print(summary)

# Obtener resumen de entrenamiento
training_summary = neural_core.get_training_summary()
print(training_summary)
```

## üíæ Guardado y Carga de Modelos

### Guardar Modelo
```python
# Guardar modelo completo
neural_core.save_model("mi_modelo.pkl")

# El archivo incluye:
# - Arquitectura de la red
# - Par√°metros entrenados
# - Historial de entrenamiento
# - Mejores pesos
# - Configuraci√≥n
```

### Cargar Modelo
```python
# Crear nueva instancia
new_neural_core = NeuralCore()

# Cargar modelo
new_neural_core.load_model("mi_modelo.pkl")

# Usar inmediatamente
predictions = new_neural_core.predict(X_new)
```

## üß™ Testing y Validaci√≥n

### Ejecutar Demostraci√≥n
```bash
python demo_red_neuronal.py
```

### Tests Unitarios
```python
# Ejecutar tests espec√≠ficos
python -m pytest celebro/red_neuronal/tests/
```

### Benchmark de Rendimiento
```python
# Ejecutar benchmark
python celebro/red_neuronal/benchmark.py
```

## üîç Casos de Uso

### 1. **Clasificaci√≥n de Im√°genes**
- Reconocimiento de d√≠gitos (MNIST)
- Clasificaci√≥n de objetos (CIFAR-10)
- Detecci√≥n de rostros
- An√°lisis m√©dico de im√°genes

### 2. **Procesamiento de Texto**
- Clasificaci√≥n de sentimientos
- An√°lisis de spam
- Categorizaci√≥n de documentos
- Traducci√≥n autom√°tica

### 3. **An√°lisis de Series Temporales**
- Predicci√≥n de precios
- An√°lisis de tendencias
- Detecci√≥n de anomal√≠as
- Pron√≥sticos meteorol√≥gicos

### 4. **Sistemas de Recomendaci√≥n**
- Filtrado colaborativo
- Filtrado basado en contenido
- Sistemas h√≠bridos
- Personalizaci√≥n

### 5. **An√°lisis de Datos**
- Regresi√≥n multivariable
- Clasificaci√≥n multiclase
- Clustering
- Reducci√≥n de dimensionalidad

## ‚ö° Optimizaciones de Rendimiento

### 1. **Vectorizaci√≥n**
- Uso eficiente de NumPy
- Operaciones vectorizadas
- Minimizaci√≥n de bucles Python

### 2. **Gesti√≥n de Memoria**
- Liberaci√≥n autom√°tica de memoria
- Reutilizaci√≥n de buffers
- Gesti√≥n eficiente de gradientes

### 3. **Paralelizaci√≥n**
- Entrenamiento por lotes
- Procesamiento paralelo de capas
- Optimizaci√≥n de operaciones matriciales

### 4. **Caching**
- Cache de activaciones
- Reutilizaci√≥n de c√°lculos
- Optimizaci√≥n de acceso a datos

## üõ†Ô∏è Desarrollo y Extensi√≥n

### Agregar Nueva Capa
```python
class MiCapaPersonalizada(Layer):
    def __init__(self, parametros, name=None):
        super().__init__(name)
        # Inicializar par√°metros
    
    def initialize_parameters(self, input_shape):
        # Inicializar par√°metros de la capa
        pass
    
    def forward(self, inputs):
        # Implementar propagaci√≥n hacia adelante
        return output
    
    def backward(self, gradients):
        # Implementar propagaci√≥n hacia atr√°s
        return input_gradients
```

### Agregar Nuevo Optimizador
```python
class MiOptimizador(Optimizer):
    def __init__(self, learning_rate=0.001, **kwargs):
        super().__init__(learning_rate)
        # Inicializar par√°metros del optimizador
    
    def update(self, parameters, gradients):
        # Implementar algoritmo de actualizaci√≥n
        return updated_parameters
```

### Agregar Nueva Funci√≥n de P√©rdida
```python
class MiFuncionP√©rdida(LossFunction):
    def __init__(self, **kwargs):
        super().__init__()
        # Inicializar par√°metros
    
    def compute(self, y_true, y_pred):
        # Calcular p√©rdida
        return loss
    
    def gradient(self, y_true, y_pred):
        # Calcular gradiente
        return gradients
```

## üìä M√©tricas y Monitoreo

### M√©tricas de Entrenamiento
- **P√©rdida de entrenamiento**: Error en datos de entrenamiento
- **P√©rdida de validaci√≥n**: Error en datos de validaci√≥n
- **Precisi√≥n**: Porcentaje de predicciones correctas
- **F1-Score**: Media arm√≥nica de precisi√≥n y recall
- **AUC-ROC**: √Årea bajo la curva ROC

### M√©tricas de Regresi√≥n
- **MSE**: Error cuadr√°tico medio
- **RMSE**: Ra√≠z del error cuadr√°tico medio
- **MAE**: Error absoluto medio
- **R¬≤**: Coeficiente de determinaci√≥n
- **MAPE**: Error porcentual absoluto medio

### Monitoreo en Tiempo Real
- **Logging detallado** de cada √©poca
- **M√©tricas en tiempo real** durante entrenamiento
- **Alertas autom√°ticas** por overfitting
- **Visualizaci√≥n** de curvas de aprendizaje

## üîí Consideraciones de Seguridad

### Validaci√≥n de Entrada
- **Verificaci√≥n de tipos** de datos
- **Validaci√≥n de rangos** de par√°metros
- **Sanitizaci√≥n** de entradas del usuario
- **L√≠mites de memoria** y procesamiento

### Protecci√≥n de Modelos
- **Cifrado** de modelos guardados
- **Verificaci√≥n de integridad** de archivos
- **Control de acceso** a modelos
- **Auditor√≠a** de uso de modelos

### Privacidad de Datos
- **Anonimizaci√≥n** de datos sensibles
- **Cifrado** de datos en tr√°nsito
- **Eliminaci√≥n segura** de datos temporales
- **Cumplimiento** de regulaciones de privacidad

## üöÄ Roadmap Futuro

### Versi√≥n 0.7.0
- [ ] **GPU Support** con CUDA
- [ ] **Distributed Training** multi-GPU
- [ ] **AutoML** para selecci√≥n autom√°tica de hiperpar√°metros
- [ ] **Neural Architecture Search** (NAS)

### Versi√≥n 0.8.0
- [ ] **Transfer Learning** pre-entrenado
- [ ] **Federated Learning** distribuido
- [ ] **Quantization** para modelos ligeros
- [ ] **Pruning** de redes neuronales

### Versi√≥n 0.9.0
- [ ] **Reinforcement Learning** integrado
- [ ] **Graph Neural Networks** (GNN)
- [ ] **Transformers** y atenci√≥n
- [ ] **Generative Models** (GANs, VAEs)

### Versi√≥n 1.0.0
- [ ] **Production Ready** con optimizaciones
- [ ] **REST API** para servicios
- [ ] **Docker** containers
- [ ] **Kubernetes** orchestration

## üìû Soporte y Contribuci√≥n

### Reportar Issues
- Usar el sistema de issues del repositorio
- Incluir logs y contexto detallado
- Especificar versi√≥n y configuraci√≥n

### Contribuir
- Fork del repositorio
- Crear rama para feature
- Pull request con descripci√≥n detallada
- Tests y documentaci√≥n incluidos

### Documentaci√≥n
- Mantener README actualizado
- Documentar cambios en CHANGELOG
- Incluir ejemplos de uso
- Tutoriales paso a paso

---

**@red_neuronal** - El sistema de redes neuronales m√°s completo y modular para LucIA. üß†‚ö°

*Desarrollado con ‚ù§Ô∏è por el equipo de LucIA Development*
