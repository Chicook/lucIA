# Red Neuronal Profunda (DNN) - M√≥dulo LucIA

## Descripci√≥n General

Este m√≥dulo implementa una Red Neuronal Profunda (DNN) completamente conectada usando TensorFlow/Keras, dise√±ada espec√≠ficamente para el proyecto LucIA. La arquitectura sigue las mejores pr√°cticas de deep learning con una estructura modular y escalable.

## Arquitectura de la Red

```
Entrada (8 caracter√≠sticas) 
    ‚Üì
Capa Densa (32 neuronas, ReLU)
    ‚Üì
Capa Densa (16 neuronas, ReLU)
    ‚Üì
Capa Densa (8 neuronas, ReLU)
    ‚Üì
Salida (4 neuronas, Softmax)
```

## Estructura de Archivos

```
RD_neuronal_LC/
‚îú‚îÄ‚îÄ modelo.py          # Definici√≥n de la arquitectura DNN
‚îú‚îÄ‚îÄ datos.py           # Generaci√≥n de datos simulados
‚îú‚îÄ‚îÄ entrenar.py        # Script principal de entrenamiento
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md         # Este archivo
```

## Caracter√≠sticas Principales

### üß† **Arquitectura Avanzada**
- **3 capas ocultas** con tama√±os decrecientes (32 ‚Üí 16 ‚Üí 8)
- **Activaci√≥n ReLU** en capas ocultas para evitar vanishing gradient
- **Activaci√≥n Softmax** en salida para clasificaci√≥n multiclase
- **Optimizador Adam** con configuraci√≥n optimizada

### üìä **Datos Simulados Realistas**
- **8 caracter√≠sticas** con distribuciones variadas (normal, exponencial, gamma, etc.)
- **4 clases** con patrones complejos y correlaciones
- **1000 muestras** balanceadas para entrenamiento
- **Normalizaci√≥n autom√°tica** de caracter√≠sticas

### üîß **Entrenamiento Robusto**
- **10 √©pocas** de entrenamiento con validaci√≥n
- **20% de datos** reservados para validaci√≥n
- **Callbacks avanzados**: Early Stopping, Reduce LR, Model Checkpoint
- **M√©tricas completas**: Accuracy, Precision, Recall

### üìà **Monitoreo y Visualizaci√≥n**
- **Gr√°ficos autom√°ticos** del historial de entrenamiento
- **Reportes detallados** con m√©tricas y estad√≠sticas
- **Logs completos** de todo el proceso
- **Guardado autom√°tico** del mejor modelo

## Instalaci√≥n

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 2. Verificar Instalaci√≥n

```bash
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
python -c "import keras; print(f'Keras: {keras.__version__}')"
```

## Uso R√°pido

### Ejecutar Entrenamiento Completo

```bash
python entrenar.py
```

### Uso Modular

```python
# Importar m√≥dulos
from modelo import crear_modelo
from datos import cargar_datos_simulados

# Cargar datos
X_train, y_train = cargar_datos_simulados()

# Crear modelo
model = crear_modelo(input_shape=8, num_classes=4)

# Mostrar resumen
model.summary()
```

## Ejemplos de Salida

### Resumen del Modelo

```
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (Dense)         (None, 32)                288       
                                                                 
 hidden_layer_1 (Dense)      (None, 16)                528       
                                                                 
 hidden_layer_2 (Dense)      (None, 8)                 136       
                                                                 
 output_layer (Dense)        (None, 4)                 36        
                                                                 
=================================================================
Total params: 988
Trainable params: 988
Non-trainable params: 0
_________________________________________________________________
```

### M√©tricas T√≠picas

```
Precisi√≥n de entrenamiento: 0.8750
Precisi√≥n de validaci√≥n: 0.8250
Mejor precisi√≥n de validaci√≥n: 0.8500
Mejor p√©rdida de validaci√≥n: 0.4123
```

## Configuraci√≥n Avanzada

### Personalizar Arquitectura

```python
# Modificar tama√±os de capas ocultas
dnn_arch = DNNArchitecture(input_shape=8, num_classes=4)
dnn_arch.hidden_layers_config = [64, 32, 16]  # Capas m√°s grandes
model = dnn_arch.create_model()
```

### Personalizar Datos

```python
# Generar m√°s muestras
X_train, y_train = cargar_datos_simulados(
    n_samples=5000,  # M√°s datos
    n_features=8,
    n_classes=4
)
```

### Personalizar Entrenamiento

```python
# Entrenamiento con m√°s √©pocas
trainer = DNNTrainer(
    epochs=50,           # M√°s √©pocas
    validation_split=0.3, # M√°s validaci√≥n
    batch_size=64        # Lotes m√°s grandes
)
```

## Archivos Generados

Despu√©s del entrenamiento se generan los siguientes archivos:

```
plots/
‚îú‚îÄ‚îÄ training_history.png    # Gr√°ficos de entrenamiento

models/
‚îú‚îÄ‚îÄ best_model.h5          # Mejor modelo guardado
‚îî‚îÄ‚îÄ dataset_entrenamiento.npz  # Dataset usado

reports/
‚îî‚îÄ‚îÄ training_report_YYYYMMDD_HHMMSS.txt  # Reporte detallado

logs/
‚îî‚îÄ‚îÄ training.log           # Logs del entrenamiento
```

## Monitoreo de Rendimiento

### M√©tricas Clave

- **Accuracy**: Precisi√≥n general del modelo
- **Precision**: Precisi√≥n por clase (macro)
- **Recall**: Sensibilidad por clase (macro)
- **Loss**: Funci√≥n de p√©rdida (categorical crossentropy)

### Indicadores de Overfitting

- Diferencia grande entre training y validation accuracy
- Validation loss que aumenta mientras training loss disminuye
- Early stopping activado frecuentemente

## Soluci√≥n de Problemas

### Error: "CUDA out of memory"

```python
# Reducir batch size
trainer = DNNTrainer(batch_size=16)  # En lugar de 32
```

### Error: "Model not converging"

```python
# Aumentar learning rate o √©pocas
optimizer = Adam(learning_rate=0.01)  # En lugar de 0.001
```

### Error: "Data shape mismatch"

```python
# Verificar formas de datos
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
```

## Contribuci√≥n

Para contribuir al m√≥dulo:

1. **Fork** el repositorio
2. **Crear** una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit** tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. **Push** a la rama (`git push origin feature/nueva-funcionalidad`)
5. **Crear** un Pull Request

## Licencia

Este m√≥dulo es parte del proyecto LucIA y est√° sujeto a la licencia del proyecto principal.

## Contacto

- **Equipo**: LucIA Development Team
- **Versi√≥n**: 1.0.0
- **Fecha**: 2025-01-11

---

## Changelog

### v1.0.0 (2025-01-11)
- ‚úÖ Implementaci√≥n inicial de DNN
- ‚úÖ M√≥dulos separados (modelo, datos, entrenar)
- ‚úÖ Generaci√≥n de datos simulados realistas
- ‚úÖ Sistema de callbacks avanzado
- ‚úÖ Visualizaci√≥n y reportes autom√°ticos
- ‚úÖ Documentaci√≥n completa

---

**üéØ Objetivo**: Proporcionar una base s√≥lida y escalable para el desarrollo de redes neuronales profundas en el ecosistema LucIA, siguiendo las mejores pr√°cticas de machine learning y deep learning.
